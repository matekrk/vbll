{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:\\Users\\mateu\\Desktop\\uni\\phd\\bayesian\\vbll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vbll\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = r\"C:\\Users\\mateu\\Desktop\\uni\\phd\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_dataset = datasets.MNIST(root='data',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "mnist_test_dataset = datasets.MNIST(root='data',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "fashion_train_dataset = datasets.FashionMNIST(root='data',\n",
    "                                    train=True,\n",
    "                                    transform=transforms.ToTensor(),\n",
    "                                    download=True)\n",
    "\n",
    "fashion_test_dataset = datasets.FashionMNIST(root='data',\n",
    "                                    train=False,\n",
    "                                    transform=transforms.ToTensor(),\n",
    "                                    download=True)\n",
    "\n",
    "def dict_to_data(dict):\n",
    "    if dict.TRAIN == \"mnist\":\n",
    "        train =  mnist_train_dataset\n",
    "    elif dict.TRAIN == \"fashion\":\n",
    "        train = fashion_train_dataset\n",
    "    if dict.TEST == \"mnist\":\n",
    "        test =  mnist_test_dataset\n",
    "    elif dict.TEST == \"fashion\":\n",
    "        test = fashion_test_dataset\n",
    "    if dict.OOD == \"mnist\":\n",
    "        ood =  mnist_test_dataset\n",
    "    elif dict.OOD == \"fashion\":\n",
    "        ood = fashion_test_dataset\n",
    "    return train, test, ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train_dataset\n",
    "test_dataset = mnist_test_dataset\n",
    "ood_dataset = fashion_test_dataset\n",
    "\n",
    "class data_cfg:\n",
    "  TRAIN = \"mnist\"\n",
    "  TEST = \"mnist\"\n",
    "  OOD = \"fashion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_performance(logs):\n",
    "  \"\"\"\n",
    "  A visualization function that plots losses, accuracies, and out of\n",
    "  distribution AUROC.\n",
    "\n",
    "  logs: a dictionary, with keys corresponding to different model evals and values\n",
    "  corresponding to dicts of results.\n",
    "  \"\"\"\n",
    "\n",
    "  # get list of colors\n",
    "  color = cm.rainbow(np.linspace(0, 1, len(logs)))\n",
    "\n",
    "  for i, (k,v) in enumerate(logs.items()):\n",
    "    # train and val loss\n",
    "    plt.plot(v['train_loss'], label=k + ' (train)', color=color[i])\n",
    "    plt.plot(v['val_loss'], label=k + ' (val)', linestyle = '--', color=color[i])\n",
    "    if 'train_loss_empirical' in v.keys():\n",
    "      plt.plot(v['train_loss_empirical'], label=k + ' (train empirical)', linestyle = 'dotted', color=color[i])\n",
    "  plt.legend()\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.show()\n",
    "\n",
    "  for i, (k,v) in enumerate(logs.items()):\n",
    "    plt.plot([1 - x for x in v['train_acc']], label=k + ' (train)', color=color[i])\n",
    "    plt.plot([1 - x for x in v['val_acc']], label=k + ' (val)', linestyle='--', color=color[i])\n",
    "\n",
    "  plt.ylabel('Error rate')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend()\n",
    "  plt.semilogy()\n",
    "  plt.show()\n",
    "\n",
    "  for i, (k,v) in enumerate(logs.items()):\n",
    "    plt.plot(v['ood_auroc'], label=k, color=color[i])\n",
    "  plt.legend()\n",
    "  plt.ylabel('OOD AUROC')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_seeds(d, k):\n",
    "  vals = np.vstack([v[k] for s,v in d.items()])\n",
    "  mean_vals, std_vals = np.mean(vals, axis=0), np.std(vals, axis=0)\n",
    "  return mean_vals, std_vals\n",
    "\n",
    "\n",
    "def viz_performance_seeds(logs, plot_title=[\"trained on ... using ...\", \"ood is ...\"]):\n",
    "  \"\"\"\n",
    "  A visualization function that plots losses, accuracies, and out of\n",
    "  distribution AUROC.\n",
    "\n",
    "  logs: a dictionary, with keys corresponding to different model evals and values\n",
    "  corresponding to dicts of results by seed.\n",
    "  \"\"\"\n",
    "\n",
    "  # get list of colors\n",
    "  color = cm.rainbow(np.linspace(0, 1, len(logs)))\n",
    "\n",
    "  plt.figure(figsize=(10,6))\n",
    "  for i, (k,v) in enumerate(logs.items()):\n",
    "    # train and val loss\n",
    "    mean_train_loss, std_train_loss = aggregate_seeds(v, 'train_loss')\n",
    "    mean_val_loss, std_val_loss = aggregate_seeds(v, 'val_loss')\n",
    "    if 'train_loss_empirical' in list(v.values())[0].keys():\n",
    "      mean_train_empirical_loss, std_train_empirical_loss = aggregate_seeds(v, 'train_loss_empirical')\n",
    "      plt.plot(mean_train_empirical_loss, label=k + ' (train empirical)', linestyle = 'dotted', color=color[i])\n",
    "      plt.fill_between(range(len(mean_train_empirical_loss)), mean_train_empirical_loss-std_train_empirical_loss, mean_train_empirical_loss+std_train_empirical_loss, alpha=0.2, color=color[i])\n",
    "      #plt.errorbar(range(len(mean_train_empirical_loss)), mean_train_empirical_loss, yerr=std_train_empirical_loss, fmt='none', ecolor=color[i], capsize=7, label='Train empirical (std)')\n",
    "    plt.plot(mean_train_loss, label=k + ' (train)', color=color[i])\n",
    "    plt.fill_between(range(len(mean_train_loss)), mean_train_loss-std_train_loss, mean_train_loss+std_train_loss, alpha=0.2, color=color[i])\n",
    "    #plt.errorbar(range(len(mean_train_loss)), mean_train_loss, yerr=std_train_loss, fmt='none', ecolor=color[i], capsize=7, label='Train (std)')\n",
    "    plt.plot(mean_val_loss, label=k + ' (val)', linestyle = '--', color=color[i])\n",
    "    plt.fill_between(range(len(mean_val_loss)), mean_val_loss-std_val_loss, mean_val_loss+std_val_loss, alpha=0.2, color=color[i])\n",
    "    #plt.errorbar(range(len(mean_val_loss)), mean_val_loss, yerr=std_val_loss, fmt='none', ecolor=color[i], capsize=7, label='Val (std)')\n",
    "  plt.legend()\n",
    "  plt.title(plot_title[0] + \" - losses\")\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure(figsize=(10,6))\n",
    "  for i, (k,v) in enumerate(logs.items()):\n",
    "    # train and val acc\n",
    "    mean_train_acc, std_train_acc = aggregate_seeds(v, 'train_acc')\n",
    "    mean_val_acc, std_val_acc = aggregate_seeds(v, 'val_acc')\n",
    "    plt.plot([1 - x for x in mean_train_acc], label=k + ' (train)', color=color[i])\n",
    "    plt.fill_between(range(len(mean_train_acc)), [1 - x - y for x,y in zip(mean_train_acc,std_train_acc)], [1 - x + y for x,y in zip(mean_train_acc,std_train_acc)], alpha=0.2, color=color[i])\n",
    "    plt.plot([1 - x for x in mean_val_acc], label=k + ' (val)', linestyle='--', color=color[i])\n",
    "    plt.fill_between(range(len(mean_val_acc)), [1 - x - y for x,y in zip(mean_val_acc,std_val_acc)], [1 - x + y for x,y in zip(mean_val_acc,std_val_acc)], alpha=0.2, color=color[i])\n",
    "\n",
    "  plt.title(plot_title[0] + \" - errors\")\n",
    "  plt.ylabel('Error rate')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend()\n",
    "  plt.semilogy()\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure(figsize=(10,6))\n",
    "  for i, (k,v) in enumerate(logs.items()):\n",
    "    # ood auroc\n",
    "    mean_ood_auroc, std_ood_auroc = aggregate_seeds(v, 'ood_auroc')\n",
    "    plt.plot(mean_ood_auroc, label=k, color=color[i])\n",
    "    plt.fill_between(range(len(mean_ood_auroc)), mean_ood_auroc-std_ood_auroc, mean_ood_auroc+std_ood_auroc, alpha=0.2, color=color[i])\n",
    "  plt.legend()\n",
    "  plt.title(plot_title[1])\n",
    "  plt.ylabel('OOD AUROC')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  \"\"\"\n",
    "  A standard MLP classification model.\n",
    "\n",
    "  cfg: a config containing model parameters.\n",
    "  \"\"\"\n",
    "  def __init__(self, cfg):\n",
    "    super(MLP, self).__init__()\n",
    "\n",
    "    self.params = nn.ModuleDict({\n",
    "      'in_layer': nn.Linear(cfg.IN_FEATURES, cfg.HIDDEN_FEATURES),\n",
    "      'core': nn.ModuleList([nn.Linear(cfg.HIDDEN_FEATURES, cfg.HIDDEN_FEATURES) for i in range(cfg.NUM_LAYERS)]),\n",
    "      'out_layer': nn.Linear(cfg.HIDDEN_FEATURES, cfg.OUT_FEATURES),\n",
    "    })\n",
    "    self.activations = nn.ModuleList([nn.ELU() for i in range(cfg.NUM_LAYERS)])\n",
    "    self.cfg = cfg\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x = self.params['in_layer'](x)\n",
    "\n",
    "    for layer, ac in zip(self.params['core'], self.activations):\n",
    "      x = ac(layer(x))\n",
    "\n",
    "    return F.log_softmax(self.params['out_layer'](x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscVBLLMLP(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super(DiscVBLLMLP, self).__init__()\n",
    "\n",
    "    self.params = nn.ModuleDict({\n",
    "      'in_layer': nn.Linear(cfg.IN_FEATURES, cfg.HIDDEN_FEATURES),\n",
    "      'core': nn.ModuleList([nn.Linear(cfg.HIDDEN_FEATURES, cfg.HIDDEN_FEATURES) for i in range(cfg.NUM_LAYERS)]),\n",
    "      'out_layer': vbll.DiscClassification(cfg.HIDDEN_FEATURES, cfg.OUT_FEATURES, cfg.REG_WEIGHT, softmax_bound=cfg.SOFTMAX_BOUND, return_empirical=cfg.RETURN_EMPIRICAL, softmax_bound_empirical=cfg.SOFTMAX_BOUND_EMPIRICAL, parameterization = cfg.PARAM, return_ood=cfg.RETURN_OOD, prior_scale=cfg.PRIOR_SCALE),\n",
    "    })\n",
    "    self.activations = nn.ModuleList([nn.ELU() for i in range(cfg.NUM_LAYERS)])\n",
    "    self.cfg = cfg\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    x = self.params['in_layer'](x)\n",
    "\n",
    "    for layer, ac in zip(self.params['core'], self.activations):\n",
    "      x = ac(layer(x))\n",
    "\n",
    "    return self.params['out_layer'](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(preds, y):\n",
    "  map_preds = torch.argmax(preds, dim=1)\n",
    "  return (map_preds == y).float().mean()\n",
    "\n",
    "def eval_ood(model, ind_dataloader, ood_dataloader, VBLL=True):\n",
    "  ind_preds = []\n",
    "  ood_preds = []\n",
    "\n",
    "  def get_score(out):\n",
    "    if VBLL:\n",
    "      score = out.ood_scores.detach().cpu().numpy()\n",
    "    else:\n",
    "      score = torch.max(out, dim=-1)[0].detach().cpu().numpy()\n",
    "    return score\n",
    "\n",
    "  for i, (x, y) in enumerate(ind_dataloader):\n",
    "    x = x.to(device)\n",
    "    out = model(x)\n",
    "    ind_preds = np.concatenate((ind_preds, get_score(out)))\n",
    "\n",
    "  for i, (x, y) in enumerate(ood_dataloader):\n",
    "    x = x.to(device)\n",
    "    out = model(x)\n",
    "    ood_preds = np.concatenate((ood_preds, get_score(out)))\n",
    "\n",
    "  labels = np.concatenate((np.ones_like(ind_preds)+1, np.ones_like(ind_preds)))\n",
    "  scores = np.concatenate((ind_preds, ood_preds))\n",
    "  fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=2)\n",
    "  return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_cfg):\n",
    "  \"\"\"Train a standard classification model with either standard or VBLL models.\n",
    "  \"\"\"\n",
    "\n",
    "  train_dataset, test_dataset, ood_dataset = dict_to_data(train_cfg.DATA)\n",
    "\n",
    "  if train_cfg.VBLL:\n",
    "    # for VBLL models, set weight decay to zero on last layer\n",
    "    param_list = [\n",
    "        {'params': model.params.in_layer.parameters(), 'weight_decay': train_cfg.WD},\n",
    "        {'params': model.params.core.parameters(), 'weight_decay': train_cfg.WD},\n",
    "        {'params': model.params.out_layer.parameters(), 'weight_decay': 0.}\n",
    "    ]\n",
    "  else:\n",
    "    param_list = model.parameters()\n",
    "    loss_fn = nn.CrossEntropyLoss() # define loss function only for non-VBLL model\n",
    "\n",
    "  optimizer = train_cfg.OPT(param_list,\n",
    "                            lr=train_cfg.LR,\n",
    "                            weight_decay=train_cfg.WD)\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size = train_cfg.BATCH_SIZE, shuffle=True)\n",
    "  val_dataloader = DataLoader(test_dataset, batch_size = train_cfg.BATCH_SIZE, shuffle=True)\n",
    "  ood_dataloader = DataLoader(ood_dataset, batch_size = train_cfg.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "  output_metrics = {\n",
    "      'train_loss': [],\n",
    "      'val_loss': [],\n",
    "      'train_acc': [],\n",
    "      'val_acc': [],\n",
    "      'ood_auroc': []\n",
    "  }\n",
    "\n",
    "  for epoch in range(train_cfg.NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    if train_cfg.VBLL and model.params.out_layer.return_empirical:\n",
    "      output_metrics['train_loss_empirical'] = []\n",
    "      running_loss_empirical = []\n",
    "    running_acc = []\n",
    "\n",
    "    for train_step, data in enumerate(train_dataloader):\n",
    "      optimizer.zero_grad()\n",
    "      x = data[0].to(device)\n",
    "      y = data[1].to(device)\n",
    "\n",
    "      out = model(x)\n",
    "      if train_cfg.VBLL:\n",
    "        loss = out.train_loss_fn(y)\n",
    "        if model.params.out_layer.return_empirical:\n",
    "          loss_empirical = out.train_loss_fn_empirical(y, train_cfg.N_SAMPLES)\n",
    "          running_loss_empirical.append(loss_empirical.item())\n",
    "        probs = out.predictive.probs\n",
    "        acc = eval_acc(probs, y).item()\n",
    "      else:\n",
    "        loss = loss_fn(out, y)\n",
    "        acc = eval_acc(out, y).item()\n",
    "\n",
    "      running_loss.append(loss.item())\n",
    "      running_acc.append(acc)\n",
    "\n",
    "      if train_cfg.VBLL and model.params.out_layer.return_empirical and train_cfg.VBLL_EMPIRICAL and train_cfg.N_SAMPLES:\n",
    "        loss_empirical.backward()\n",
    "      else:\n",
    "        loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    output_metrics['train_loss'].append(np.mean(running_loss))\n",
    "    if train_cfg.VBLL and model.params.out_layer.return_empirical:\n",
    "      output_metrics['train_loss_empirical'].append(np.mean(running_loss_empirical))\n",
    "    output_metrics['train_acc'].append(np.mean(running_acc))\n",
    "\n",
    "    if epoch % train_cfg.VAL_FREQ == 0:\n",
    "      running_val_loss = []\n",
    "      running_val_acc = []\n",
    "\n",
    "      with torch.no_grad():\n",
    "        model.eval()\n",
    "        for test_step, data in enumerate(val_dataloader):\n",
    "          x = data[0].to(device)\n",
    "          y = data[1].to(device)\n",
    "\n",
    "          out = model(x)\n",
    "          if train_cfg.VBLL:\n",
    "            loss = out.val_loss_fn(y)\n",
    "            probs = out.predictive.probs\n",
    "            acc = eval_acc(probs, y).item()\n",
    "          else:\n",
    "            loss = loss_fn(out, y)\n",
    "            acc = eval_acc(out, y).item()\n",
    "\n",
    "          running_val_loss.append(loss.item())\n",
    "          running_val_acc.append(acc)\n",
    "\n",
    "        output_metrics['val_loss'].append(np.mean(running_val_loss))\n",
    "        output_metrics['val_acc'].append(np.mean(running_val_acc))\n",
    "      output_metrics['ood_auroc'].append(eval_ood(model, val_dataloader, ood_dataloader, VBLL=train_cfg.VBLL))\n",
    "      print('Epoch: {:2d}, train loss: {:4.4f}, train acc: {:4.4f}'.format(epoch, np.mean(running_loss), np.mean(running_acc)))\n",
    "      if train_cfg.VBLL and model.params.out_layer.return_empirical:\n",
    "        print('Epoch: {:2d}, train loss empirical: {:4.4f}'.format(epoch, np.mean(running_loss_empirical)))\n",
    "      print('Epoch: {:2d}, valid loss: {:4.4f}, valid acc: {:4.4f}'.format(epoch, np.mean(np.mean(running_val_loss)), np.mean(np.mean(running_val_acc))))\n",
    "  return output_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_LR = 1e-3\n",
    "GLOBAL_LR_VBLL = 3e-3\n",
    "GLOBAL_NUM_EPOCHS = 30\n",
    "GLOBAL_OPT = torch.optim.AdamW\n",
    "GLOBAL_SEEDS = [0,1,2]\n",
    "GLOBAL_PS = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_cfg:\n",
    "  DATA = data_cfg()\n",
    "  NUM_EPOCHS = GLOBAL_NUM_EPOCHS\n",
    "  BATCH_SIZE = 512\n",
    "  LR = GLOBAL_LR\n",
    "  WD = 1e-4\n",
    "  OPT = GLOBAL_OPT\n",
    "  CLIP_VAL = 1\n",
    "  VAL_FREQ = 1\n",
    "  VBLL = False\n",
    "\n",
    "class cfg:\n",
    "  IN_FEATURES = 784\n",
    "  HIDDEN_FEATURES = 128\n",
    "  OUT_FEATURES = 10\n",
    "  NUM_LAYERS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_c = cfg()\n",
    "tr_c = train_cfg()\n",
    "exp_name = f'MLP-POINT opt:{tr_c.OPT.__name__} lr:{tr_c.LR}'\n",
    "exp_setup = f'model:{mod_c.NUM_LAYERS}x{mod_c.HIDDEN_FEATURES} data:{tr_c.DATA.TRAIN} ood:{tr_c.DATA.OOD}'\n",
    "outputs[exp_name] = dict()\n",
    "\n",
    "for s in GLOBAL_SEEDS:\n",
    "    tr_c.SEED = s\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    model = MLP(mod_c).to(device)\n",
    "    outputs[exp_name][s] = train(model, tr_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_cfg:\n",
    "  DATA = data_cfg()\n",
    "  NUM_EPOCHS = 30\n",
    "  BATCH_SIZE = 512\n",
    "  LR = GLOBAL_LR_VBLL\n",
    "  WD = 1e-4\n",
    "  OPT = torch.optim.AdamW\n",
    "  CLIP_VAL = 1\n",
    "  VAL_FREQ = 1\n",
    "  VBLL = True\n",
    "  VBLL_EMPIRICAL = False\n",
    "  N_SAMPLES = 10\n",
    "\n",
    "class cfg:\n",
    "    IN_FEATURES = 784\n",
    "    HIDDEN_FEATURES = 128\n",
    "    OUT_FEATURES = 10\n",
    "    NUM_LAYERS = 2\n",
    "    REG_WEIGHT = 1./train_dataset.__len__()\n",
    "    PARAM = 'diagonal'\n",
    "    SOFTMAX_BOUND = \"jensen\"\n",
    "    RETURN_EMPIRICAL = True\n",
    "    SOFTMAX_BOUND_EMPIRICAL = \"montecarlo\"\n",
    "    RETURN_OOD = True\n",
    "    PRIOR_SCALE = GLOBAL_PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_c = cfg()\n",
    "tr_c = train_cfg()\n",
    "exp_name = f'MLP-VBLL-PS{mod_c.PRIOR_SCALE} opt:{tr_c.OPT.__name__} lr:{tr_c.LR}'\n",
    "exp_setup = f'model:{mod_c.NUM_LAYERS}x{mod_c.HIDDEN_FEATURES} data:{tr_c.DATA.TRAIN} ood:{tr_c.DATA.OOD}'\n",
    "outputs[exp_name] = dict()\n",
    "\n",
    "for s in GLOBAL_SEEDS:\n",
    "    tr_c.SEED = s\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    model = MLP(mod_c).to(device)\n",
    "    outputs[exp_name][s] = train(model, tr_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_cfg:\n",
    "  TRAIN = \"mnist\"\n",
    "  TEST = \"mnist\"\n",
    "  OOD = \"fashion\"\n",
    "\n",
    "class train_cfg:\n",
    "  DATA = data_cfg()\n",
    "  NUM_EPOCHS = 30\n",
    "  BATCH_SIZE = 512\n",
    "  LR = GLOBAL_LR_VBLL\n",
    "  WD = 1e-4\n",
    "  OPT = torch.optim.AdamW\n",
    "  CLIP_VAL = 1\n",
    "  VAL_FREQ = 1\n",
    "  VBLL = True\n",
    "  VBLL_EMPIRICAL = True\n",
    "  N_SAMPLES = 10\n",
    "\n",
    "class cfg:\n",
    "    IN_FEATURES = 784\n",
    "    HIDDEN_FEATURES = 128\n",
    "    OUT_FEATURES = 10\n",
    "    NUM_LAYERS = 2\n",
    "    REG_WEIGHT = 1./mnist_train_dataset.__len__()\n",
    "    PARAM = 'diagonal'\n",
    "    SOFTMAX_BOUND = \"jensen\"\n",
    "    RETURN_EMPIRICAL = True\n",
    "    SOFTMAX_BOUND_EMPIRICAL = \"montecarlo\"\n",
    "    RETURN_OOD = True\n",
    "    PRIOR_SCALE = GLOBAL_PS\n",
    "\n",
    "disc_vbll_model = DiscVBLLMLP(cfg()).to(device)\n",
    "outputs[f'DiscVBLL empirical {train_cfg.N_SAMPLES}'] = train(disc_vbll_model, train_cfg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_c = cfg()\n",
    "tr_c = train_cfg()\n",
    "exp_name = f'MLP-VBLL-PS{mod_c.PRIOR_SCALE}-MC{tr_c.N_SAMPLES} opt:{tr_c.OPT.__name__} lr:{tr_c.LR}'\n",
    "exp_setup = f'model:{mod_c.NUM_LAYERS}x{mod_c.HIDDEN_FEATURES} data:{tr_c.DATA.TRAIN} ood:{tr_c.DATA.OOD}'\n",
    "outputs[exp_name] = dict()\n",
    "\n",
    "for s in GLOBAL_SEEDS:\n",
    "    tr_c.SEED = s\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    model = DiscVBLLMLP(mod_c).to(device)\n",
    "    outputs[exp_name][s] = train(model, tr_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_c = cfg()\n",
    "tr_c = train_cfg()\n",
    "tr_c.N_SAMPLES = 1000\n",
    "exp_name = f'MLP-VBLL-PS{mod_c.PRIOR_SCALE}-MC{tr_c.N_SAMPLES} opt:{tr_c.OPT.__name__} lr:{tr_c.LR}'\n",
    "exp_setup = f'model:{mod_c.NUM_LAYERS}x{mod_c.HIDDEN_FEATURES} data:{tr_c.DATA.TRAIN} ood:{tr_c.DATA.OOD}'\n",
    "outputs[exp_name] = dict()\n",
    "\n",
    "for s in GLOBAL_SEEDS:\n",
    "    tr_c.SEED = s\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    model = DiscVBLLMLP(mod_c).to(device)\n",
    "    outputs[exp_name][s] = train(model, tr_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_c = cfg()\n",
    "tr_c = train_cfg()\n",
    "mod_c.PRIOR_SCALE = 10000\n",
    "exp_name = f'MLP-VBLL-PS{mod_c.PRIOR_SCALE}-MC{tr_c.N_SAMPLES} opt:{tr_c.OPT.__name__} lr:{tr_c.LR}'\n",
    "exp_setup = f'model:{mod_c.NUM_LAYERS}x{mod_c.HIDDEN_FEATURES} data:{tr_c.DATA.TRAIN} ood:{tr_c.DATA.OOD}'\n",
    "outputs[exp_name] = dict()\n",
    "\n",
    "for s in GLOBAL_SEEDS:\n",
    "    tr_c.SEED = s\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    model = DiscVBLLMLP(mod_c).to(device)\n",
    "    outputs[exp_name][s] = train(model, tr_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_performance_seeds(outputs, [f\"[sgd] trained on {data_cfg().TRAIN}\", f\"[sgd] OOD is {data_cfg.OOD}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_performance_seeds(outputs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
